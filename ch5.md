# Ch.5 νΈλ¦¬ μ•κ³ λ¦¬μ¦
## 1. κ²°μ • νΈλ¦¬
![plot_tree](image-4.png)
```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```
- `plot_tree()` : κ²°μ •νΈλ¦¬λ¥Ό νΈλ¦¬ κ·Έλ¦ΌμΌλ΅ μ¶λ ¥
λ„λ¬΄ λ³µμ΅!! β΅οΈ **ν•μ΄νΌνλΌλ―Έν„° μμ •**

![max_depth](image-5.png)
```python
plot_tree(dt, max_depth=1, filled=True, feature_names=['ν–‰1', 'ν–‰2', 'ν–‰3'])
plt.show()
```
- `label='all'/'root'/'none'`(κΈ°λ³Έκ°’μ€ all) : ν‘μ‹ν•  ν…μ¤νΈ μΆ…λ¥
  - ν…μ¤νΈ μ΅°κ±΄ ex. sugar <= -0.239
  - gini μ§€λ‹λ¶μλ„ 
  - samples μ΄ μƒν” μ
  - value ν΄λμ¤λ³„ μƒν” μ 
- `filled=True` : λ…Έλ“μ λ°°κ²½μƒ‰ μ—¬λ¶€; μ–΄λ–¤ ν΄λμ¤μ λΉ„μ¨μ΄ λ†’μ•„μ§€λ©΄ μƒ‰μ΄ μ§„ν•΄μ§

β“ **gini μ§€λ‹ λ¶μλ„**
![gini](image-6.png)
- gini = 0.5 : λ…Έλ“μ λ‘ ν΄λμ¤ λΉ„μ¨μ΄ μ •ν™•ν 1/2μ”© => μµμ•…!
- gini = 0 : λ…Έλ“μ— ν•λ‚μ ν΄λμ¤λ§ μλ”κ²ƒ(μμλ…Έλ“)
1. μ΄μ§„λ¶„λ¥μΈ κ²½μ°
0(μμλ…Έλ“) <= gini(G) <= 0.5 (λ°λ°)
2. λ‹¤μ¤‘ν΄λμ¤μΈ κ²½μ°
0 <= gini(G) < 1 (0.5λ³΄λ‹¤ μ»¤μ§ μ μμ)

```text
κ²°μ •νΈλ¦¬κ°€ λ…Έλ“λ¥Ό λ‚λ„λ” λ°©λ²•
π’΅ λ¶€λ¨μ™€ μμ‹ λ…Έλ“ μ‚¬μ΄μ λ¶μλ„ μ°¨μ΄(μ •λ³΄ μ΄λ“)μ΄ μµλ€κ°€ λλ„λ΅
>> λ¶μλ„μ— λ”°λΌ λ‚λ„κΈ° λ•λ¬Έμ— β€ΌοΈν‘μ¤€ν™” μ „μ²λ¦¬ λ¶ν•„μ”β€ΌοΈ
```

β“ **entropy μ—”νΈλ΅ν”Ό**
![entropy](image-7.png)
```
gini vs entropy
> μ†λ„μ™€ μ΄λ΅ μ  ν•΄μ„μ μ°¨μ΄κ°€ μμΌλ‚, κ²°μ • νΈλ¦¬μ—μ„ ν° μ°¨μ΄ μ—†μ
> DecisionTreeClassifierμ default λ” gini
```

**feature_importances**
```python
print(dt.feature_importances_)
```
- νΉμ„± μ¤‘μ”λ„μ ν¨κ³Ό?λ¥Ό μ λ» λκΌλ”λ° ν™•μ‹¤ν κ²°μ •νΈλ¦¬μ—μ„λ” ν΄λμ¤λ¥Ό λ¶„λ¥ν•λ”λ° μ–΄λ–¤ νΉμ„±μ΄ μν–¥μ„ λ§μ΄ λ―Έμ³¤λ”μ§€ λ³΄κ³  ν•΄μ„ν•  μ μκ² κµ¬λ‚..~!

## 2. κµμ°¨ κ²€μ¦κ³Ό κ·Έλ¦¬λ“ μ„μΉ
```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)
dt = gs.best_estimator_
```
- GridSearch λ¥Ό μ΄μ©ν•λ©΄ ν•μ΄νΌνλΌλ―Έν„° & λ¨λΈ νλΌλ―Έν„° μµμ κ°’ μ°ΎκΈ° ν• λ²μ— κ°€λ¥
- `min_impurity_decrease` : λ…Έλ“λ¥Ό λ¶„ν• ν•κΈ° μ„ν• λ¶μλ„ κ°μ† μµμ†λ‰ 

β΅οΈ λ” λ³µμ΅ν•κ² 
```python
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001), 'max_depth': range(5, 20, 1), 'min_samples_split': range(2, 100, 10)}
```
- `range(μ«μ1, μ«μ2, μ«μ3)` : μ«μ1μ—μ„ μ‹μ‘ν•΄μ„ μ«μ2κ°€ λ  λ•κΉμ§€ μ«μ3μ”© μ¦κ°€ν•λ©΄μ„ λ§¤κ°λ³€μ μ΅°μ • 

β΅οΈ λ§¤κ°λ³€μ κ°’ λ²”μ„λ‚ κ°„κ²© μ •ν•κΈ° μ–΄λ ¤μΈ λ• / λ„λ¬΄ λ§μ€ λ§¤κ°λ³€μ μ΅°κ±΄μ΄ μμ„ λ•

**λλ¤ μ„μΉ** : λ§¤κ°λ³€μλ¥Ό μƒν”λ§ν•  μ μλ” ν™•λ¥  λ¶„ν¬ κ°μ²΄ μ „λ‹¬
```python
from scipy.stats import uniform, randint 
params = {'min_impurity_decrease': uniform(0.0001, 0.001), 'max_depth': randint(20, 50), 'min_samples_split': randint(2, 25), 'min_samples_leaf': randint(1, 25),}

from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)
```

## 3. νΈλ¦¬μ μ•™μƒλΈ”
### 1) λλ¤ ν¬λ μ¤νΈ Random Forest 
```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
#### μ‘λ™μ›λ¦¬ 
- ν›λ ¨ λ°μ΄ν„°λ¥Ό λ¶€νΈμ¤νΈλ© μƒν”λ§(μ¤‘λ³µν•΄μ„ λλ¤ μ¶”μ¶) ;λ‹¨, ν›λ ¨ μƒν”κ³Ό ν¬κΈ° κ°™κ² μƒν”λ§

-> RandomForestClassifier(μ „μ²΄ νΉμ„± κ°μμ μ κ³±κ·Όλ§νΌ νΉμ„± μ„ νƒ)/RandomForestRegressor(μ „μ²΄ νΉμ„± μ‚¬μ©)

-> **λ¶„λ¥** : κΈ°λ³Έμ μΌλ΅ 100κ°μ κ²°μ • νΈλ¦¬λ¥Ό μ΄λ° λ°©μ‹μΌλ΅ ν›λ ¨ ν›„ κ° νΈλ¦¬μ ν΄λμ¤λ³„ ν™•λ¥ μ„ ν‰κ· ν•μ—¬ κ°€μ¥ λ†’μ€ ν™•λ¥ μ„ κ°€μ§„ ν΄λμ¤λ¥Ό μμΈ΅μΌλ΅ μ‚Όμ / **νκ·€** : λ‹¨μν κ° νΈλ¦¬μ μμΈ΅ ν‰κ· 

#### μ¥μ 
λλ¤ν•κ² μ„ νƒν• μƒν” & νΉμ„± μ‚¬μ© => ν›λ ¨ μ„ΈνΈ κ³Όλ€μ ν•© λ°©μ§€ + μΌλ°ν™” μ„±λ¥ λ†’μ„ 

#### OOB μƒν”
```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```
- λ¶€νΈμ¤νΈλ© μƒν”μ— ν¬ν•¨λμ§€ μ•μ€ λ‚¨μ€ μƒν”(out of bag sample)
- κ²€μ¦μ„ΈνΈμ μ—­ν•  μν–‰ κ°€λ¥ => λ³„λ„μ κ²€μ¦μ„ΈνΈλ¥Ό λ¶„λ¦¬ν•  ν•„μ”κ°€ μ—†μΌλ―€λ΅ **ν›λ ¨ μ„ΈνΈμ— λ” λ§μ€ μƒν” μ‚¬μ© κ°€λ¥**

### 2) μ—‘μ¤νΈλΌ νΈλ¦¬ Extra Trees 
#### μ‘λ™μ›λ¦¬ 
- λ¶€νΈμ¤νΈλ© μƒν”λ§μ„ μ‚¬μ©ν•μ§€ μ•κ³  μ „μ²΄ ν›λ ¨ μ„ΈνΈλ΅ κ²°μ • νΈλ¦¬ λ§λ“¦
- λ…Έλ“ λ¶„ν• μ‹ κ°€μ¥ μΆ‹μ€ λ¶„ν•  μ°ΎκΈ° x. **λ¬΄μ‘μ„ λ¶„ν• **(μ„κ³„κ°’ μμ²΄κ°€ λ¬΄μ‘μ„)

```text
 κ³Όμ ν•© λ°©μ§€ν•κ³  μ•μ •μ„±μ„ λ†’μΌλ¦¬λ©΄ λλ¤ν¬λ μ¤νΈ, 
 λΉ λ¥Έ μ†λ„λ¥Ό μ›ν•λ©΄ μ—‘μ¤νΈλΌ νΈλ¦¬
 ```

 ### 3) κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν… Gradient Boosting
 ```python
 from sklearn.ensemble import GradientBoostingClassifier
 gb = GradientBoostingClassifier(random_state=42)
 scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
 print(np.mean(scores['train_socre']), np.mean(scores['test_score']))
 ```

 #### μ‘λ™μ›λ¦¬
 - κΉμ΄κ°€ μ–•μ€ κ²°μ •νΈλ¦¬λ¥Ό μ‚¬μ©ν•μ—¬ μ΄μ „ νΈλ¦¬μ μ¤μ°¨λ¥Ό λ³΄μ™„ν•λ” λ°©μ‹ 

#### λ§¤κ°λ³€μ subsample
- νΈλ¦¬ ν›λ ¨μ— μ‚¬μ©ν•  ν›λ ¨ μ„ΈνΈμ λΉ„μ¨ μ •ν•¨
  - `subsample=1` : μ „μ²΄ ν›λ ¨ μ„ΈνΈ μ‚¬μ©
  - `subsample<1` : ν›λ ¨ μ„ΈνΈμ μΌλ¶€ μ‚¬μ© (ν™•λ¥ μ  κ²½μ‚¬ ν•κ°•λ²• λλ” λ―Έλ‹λ°°μΉ κ²½μ‚¬ ν•κ°•λ²•κ³Ό μ μ‚¬)

### 4) νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν… Histogram-based Gradient Boosting
#### μ‘λ™ μ›λ¦¬
- μ…λ ¥ νΉμ„±μ„ 256κ°μ κµ¬κ°„μΌλ΅ λ‚λ” => μµμ μ λ¶„ν• μ„ λΉ λ¥΄κ² μ°Ύμ„ μ μμ 

#### `permutation_importance()`
- νΉμ„± μ¤‘μ”λ„ κ³„μ‚°
  - νΉμ„±μ„ ν•λ‚μ”© λλ¤ν•κ² μ„μ–΄μ„ λ¨λΈ μ„±λ¥ λ³€ν™” κ΄€μ°° ν›„ μ¤‘μ”λ„ κ³„μ‚° (ν›λ ¨, ν…μ¤νΈ μ„ΈνΈ μ „λ¶€ μ‚¬μ© κ°€λ¥)

### XGBoost λΌμ΄λΈλ¬λ¦¬
- Scikit learn λΌμ΄λΈλ¬λ¦¬κ°€ μ•„λ‹λ”λΌλ„ κ·Έλ λ””μ–ΈνΈ λ¶€μ¤ν… μ•κ³ λ¦¬μ¦ κµ¬ν„ κ°€λ¥
  - `tree_method = hist` λ΅ ν•  κ²½μ° νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ λ””μ–ΈνΈ λ¶€μ¤ν… 
- LightGBM λ„ μ‚¬μ© κ°€λ¥(`LGBMClassifier`)

## cf
```python
df.describe()
```
- μ‹κ°ν™”κ°€ μ•„λ‹λΌ describe λ¥Ό ν™μ©ν•΄μ„ μ¤μΌ€μΌ ν™•μΈν•κ³  scaler μ‚¬μ© μ—¬λ¶€λ¥Ό κ²°μ •ν•΄λ„ λ¨

```python
data = df.[['ν–‰1', 'ν–‰2', 'ν–‰3', ...]].to_numpy()
target = df['νƒ€κ²λ³€μ'].to_numpy()
```
- Scikit-learn κ°™μ€ ML λΌμ΄λΈλ¬λ¦¬λ“¤μ΄ NumPy λ°°μ—΄μ„ μ…λ ¥λ°›λ„λ΅ μ„¤κ³„λμ–΄ μκΈ° λ•λ¬Έμ— numpy λ°°μ—΄λ΅ λ³€κ²½ εΏ…
```python
random state = 00
```
- random state μ«μκ°€ κ°™μ„ κ²½μ° λ™μΌν• λλ¤ λ°μ΄ν„°λ΅ ν›λ ¨
- μ‹¤μ „μ—μ„λ” ν•„μ”ν•μ§€ μ•μ§€λ§, μ—°μµ λλ” μ½”λ“ κ³µμ  μ‹μ— κ°™μ€ μ«μλ΅ ν•λ©΄ λ™μΌν• μ μλ¥Ό μ–»μ„ μ μμ

`scipy(μ‹Έμ΄νμ΄) λΌμ΄λΈλ¬λ¦¬` : μ λ¶„, λ³΄κ°„, μ„ ν•λ€μ, ν™•λ¥  λ“±μ„ ν¬ν•¨ν• μμΉ κ³„μ‚° μ „μ© λΌμ΄λΈλ¬λ¦¬(μ‹Έμ΄ν‚·λ°μ΄ λ„νμ΄μ™€ μ‹Έμ΄νμ΄ κΈ°λ¥μ„ λ§μ΄ μ‚¬μ©ν•¨)

```python
from scipy.stats import uniform, randint
rgen = randint(0, 10)
np.unique(rgen.rvs(1000), return_counts=True)
```
- μ •μ 10κ°μ”© λ¬¶μ—¬μλ” λ°°μ—΄ 1000κ°λ¥Ό λ½‘μ
- `randint` μλ¦¬μ— `uniform` λ„£μΌλ©΄ μ‹¤μ 10κ°μ”© λ¬¶μ—¬μλ λ°°μ—΄ 1000κ° λ½‘μ